<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Towards Uncertainty-Aware Language Agent"/>
  <meta property="og:description" content="Uncertainty-Aware Language Agent (UALA)"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Protecting Your LLMs with Information Bottleneck</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet"> -->

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://kit.fontawesome.com/deb78776bf.js" crossorigin="anonymous"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Protecting Your LLMs with Information Bottleneck</h1>
            <br>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Zichuan Liu,</span>
              <span class="author-block">
                Zefan Wang,</span>
              <span class="author-block">
                Linjie Xu,</span>
              <span class="author-block">
                Jinyu Wang,</span>
                <p></p>
              <span class="author-block">
                Lei Song,</span>
               <span class="author-block">
                Tianchun Wang,</span>
              <span class="author-block">
                Chunlin Chen,</span>
              <span class="author-block">
                Wei Cheng,</span>
              <span class="author-block">
                Jiang Bian</span>
                </span>
            </div>

                  <div class="is-size-5 publication-authors">
                  	<p></p>
                    <span class="author-block">Nanjing University, Microsoft Research Asia, <br>Tsinghua University, Queen Mary University of London, <br>Pennsylvania State University, NEC Laboratories America
</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2404.13968.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary Model link -->
                    <!-- <span class="link-block">
                      <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa-regular fa-box-archive"></i>
                      </span>
                      <span>Model</span>
                    </a>
                  </span> -->

                    <!-- Supplementary data link -->
                    <span class="link-block">
                      <a href="https://github.com/zichuan-liu/IB4LLMs/blob/main/intro_slides_llm.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Slides</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/zichuan-liu/IB4LLMs" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->

<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content.
Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts.
To address this, we introduce the Information Bottleneck Protector (IBProtector),
a defense mechanism grounded in the information bottleneck principle, and we
modify the objective to avoid trivial solutions. The IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor,
preserving only essential information for the target LLMs to respond with the
expected answer. Moreover, we further consider a situation where the gradient is
not visible to be compatible with any LLM. Our empirical evaluations show that
IBProtector outperforms current defense methods in mitigating jailbreak attempts,
without overly affecting response quality or inference speed. Its effectiveness
and adaptability across various attack methods and target LLMs underscore the
potential of IBProtector as a novel, transferable defense that bolsters the security
of LLMs without requiring modifications to the underlying models.
          </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-justified">
		The random perturbations might not consistently target tokens triggering jailbreaks. Can we perturb adversarial prompts more effectively beyond mere randomness?
      </h2>      <div class="item has-text-centered">
      <img src="static/images/intro.png" alt="Teaser image" class="teaser-image" width="70%">
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-justified">
Our framework of IBProtector, where fire and snowflake denote frozen and trained
parameters, respectively, and the small language model is optional. Given an input prompt, the
extractor can extract the most informative parts for the predictor to respond to.         </h2>      <div class="item has-text-centered">
      <img src="static/images/model.png" alt="Teaser image" class="teaser-image" width="70%">
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{liu2024protecting,
      title={Protecting Your LLMs with Information Bottleneck}, 
      author={Zichuan Liu and Zefan Wang and Linjie Xu and Jinyu Wang and Lei Song and Tianchun Wang and Chunlin Chen and Wei Cheng and Jiang Bian},
  journal={arXiv preprint arXiv:2404.13968},
  year={2404}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
